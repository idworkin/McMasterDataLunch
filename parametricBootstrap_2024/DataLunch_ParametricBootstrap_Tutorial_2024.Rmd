---
title: "Tutorial for the parametric bootstrap with mixed effect models"
author: "Ian Dworkin"
date: "`r format(Sys.time(),'%d %b %Y')`"
output:
  html_document: 
    toc: yes
    fig_caption: yes
    keep_md: yes
    number_sections: yes
  pdf_document: 
    toc: yes
    number_sections: yes
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(list(digits = 4, show.signif.stars = F, show.coef.Pvalues = TRUE)) # Sadly we need to show p-values to discuss p-values...
```

# Using the parametric bootstrap (Monte Carlo simulations) for inferences with mixed models


## libraries
```{r}
library(lme4)
library(glmmTMB)
library(pbkrtest)
library(MuMIn)
library(car)
library(r2glmm) # mixed model r^2 etc..
library(lattice)
library(corrplot)
```



## read in data, and prep it.
```{r}
sct_data  <- read.csv("https://raw.githubusercontent.com/DworkinLab/DworkinLab.github.io/master/dataSets/Dworkin2005_ED/dll.csv", 
                    header=TRUE,
                    stringsAsFactors = TRUE)

sct_data <- na.omit(sct_data)

sct_data$temp <- as.factor(sct_data$temp)
sct_data$replicate <- factor(sct_data$replicate)

sct_data$tarsus.centered <- scale(sct_data$tarsus, 
                                  center = T, scale = T) 

sct_data$genotype <- relevel(sct_data$genotype, "wt")
```



## Let's get a sense of the question we are thinking about

- These values will only give us a rough idea, but provides a ballpark for the estimates.

```{r}
SCT_strain_means <- with(sct_data,
                         tapply(SCT, list(line, genotype), mean))

plot(SCT_strain_means, 
     col = "red", pch = 20, cex = 2,
     xlim = c(9.5,15), ylim = c(9.5, 15), 
     main = "average SCT number by strain, across two genotypes")

abline(a = 0, b = 1, lty = 2, col = rgb(0,0,1, 0.25))

cor(SCT_strain_means) # correlation among strain means across the two genotypes

apply(SCT_strain_means, 2, sd) # among strain standard deviation for SCT for the two genotypes
```



## Let's model this formally in a mixed model

- note I am using `(0 + genotype | line)` notation for the random effect as I interested in the variances of the random effects themselves, not expressed as a treatment contrast (like we have for the fixed effects).


```{r}
model_1_REML_us <- lmer(SCT ~ 1 +  genotype + (0 + genotype | line), 
                      data = sct_data) # NOTE: REML = TRUE by default
```


```{r}
summary(model_1_REML_us)

VarCorr(model_1_REML_us)
```


Let's examine the conditional means (the "strain" means), by genotype

```{r}
plot(ranef(model_1_REML_us), 
     pch = 20, col = "blue", cex = 2,
     main = "conditional means")
```

##  confidence intervals via profiling the likelihood surface

```{r}
model1_ML_profile <- profile(model_1_REML_us, signames = FALSE) # profile the likelihood surface

confint(model1_ML_profile) # For more complicated models this may not work. So you may need to use parametric or non-parametric bootstrap in particular for the variances.

# .sig01 corresponds to the StDDev for the random intercept (among strain variance for SCT number for wild type)
# .sig02 corresponds to the StDDev for the among strain correlation across the two genotypes
# .sig03 corresponds to the StDDev for the genotypeDll
# .sigma is the residual standard deviation
```

## plotting to get a sense of the confidence intervals

```{r}
lattice::xyplot(model1_ML_profile, absVal = TRUE, conf = c(0.95),
       main = "95% profile() intervals", lty = 2)
```


```{r}
lattice::densityplot(model1_ML_profile, upper = 0.975)
```

## Percent variance accounted for

A bit tricky in mixed, models, but there are some approaches to it.

MUMin marginal (fixed) and condition (fixed + random R2 measures for model)
```{r}
r.squaredGLMM(model_1_REML_us)

r2beta(model_1_REML_us)
```



## degrees of freedom in mixed models is not simple!
Keep in mind that "counting" parameters for random effects is difficult and may be done differently by different programs

```{r}
-2*logLik(model_1_REML_us)
```

## Model "testing


Generally they only assess fit of fixed effect terms (conditioned on the random terms), but do not assess differences in models that differ for random effects.  I think the library `afex` has some related functionality.
```{r}
car::Anova(model_1_REML_us)
```


### testing variance components
If for whatever reason the confidence intervals on the variance components are not enough, you can use some approaches based on ideas from a likelihood ratio (LR) to compare models with and without a particular random effect (make sure REML = TRUE).  As we discussed in class, most of the time the LR in the context of a likelihood ratio test (LRT), i.e. comparing the LR in the context of a $\chi^2$ distribution is not the way to go. Most often we will use either permutation tests or a parametric bootstrap.

Counting the number of parameters for random effects can be difficult if the data is not balanced. Therefore it is best to use "test statistics" which will not depend on knowing the number of random effect parameters being estimated (as a df). So commonly you will see (and this is what pbkrtest does) a likelihood ratio, but evaluated using parametric bootstrap, permutation or non-parametric bootstrap (the latter two take some seriously thinking about the resampling procedure).



I have not used `pbkrtest` library much, and generally write my own parametric bootstrap simulator either like we did in class, or using simulate(yourMerModel). `simulate()` is a generic function that calls class specific methods, in this case (for `lmer`) `simulate.merMod`.


```{r}
logLik(model_1_REML_us)
REMLcrit(model_1_REML_us) # the deviance, i.e. $2 X L$
```

### reduced model

If we wanted to compare to the simpler model without the "random slope" for genotypic effects (and also the correlation between random slope and intercept in this case) we could fit the following model.

```{r}
model_reduced_REML <- lmer(SCT ~ 1 + genotype + (1| line), 
                      data = sct_data) 

logLik(model_reduced_REML)
```


How do we compare?  We can compute the Likelihood Ratio (LR) between models (below)...
 
```{r}
LR_model <-  -as.numeric(REMLcrit(model_1_REML_us) - REMLcrit(model_reduced_REML))

LR_model
```

### A pure Likelihood ratio test (LRT) can get you in trouble

The question now becomes how many df these two models differ by. We are estimating 2 less terms for the random effects (the random "slope" for the genotypic effect, and covariance between the random slope and the random intercept). So one approach would be to compare this to a $\chi^2$ distribution with 2 df.

```{r}
pchisq(q = LR_model, df = 2, lower = F) 
```

Which is very similar to as the default anova method (although this refits under ML not REML).

```{r}
anova(model_1_REML_us, model_reduced_REML)
```

However, as a reminder, we previously discussed issues with LRT near boundary conditions (in this case a variance of 0), and whether this approach is at all appropriate. 

We also have the issues about degrees of freedom. What do I mean? Instead of thinking about the the 2 additional parameters for the random effects (the random slope and covariance between it and the random intercept), we could instead frame this around the 27 additional "conditional means"/BLUPs we are computing. So how do we know which (if either) we should use? There are approaches that try to deal with this issue (estimating the df), but we have better tools at our disposal that allow us to not have to worry about this issue.

So even putting aside the issues with the LRT for a boundary condition (a variance of 0), do we use the LRT with the above df, or the following?

```{r}
pchisq(q = LR_model, df = 27, lower = F) 
```

 you can see by the massive change in the magnitude of the p-value.  Best to rely on the parametric bootstrap! It does not require information about degrees of freedom or making assumptions necessary for the LRT based on $\chi^2$


Remember that the model we are using for the simulation is the reduced model, even though we refit (with the simulated data) under both the full and reduced models!

```{r}
LikRatioSim <- function(mod = model_reduced_REML) {
	y_sim <- simulate(mod, nsim = 1)  # one set of simulated data under reduced model
	model_lower <- lmer(y_sim$sim_1 ~ 1 + genotype + (1|line), data = sct_data) # fit simulated data under reduced model
	
	model_full  <- lmer(y_sim$sim_1 ~ 1 + genotype + (0 + genotype|line), data = sct_data) # fit simulated data w complex model 
	
	LRSim <-  -as.numeric(REMLcrit(model_full) - REMLcrit(model_lower)) # Likelihood Ratio
	return(LRSim)
}
```

Test that it works (should spit out one number for each run, and the numbers should be different each time)
```{r, message=FALSE, error=FALSE}
LikRatioSim()
LikRatioSim()
LikRatioSim()
```


```{r, message=FALSE, error=FALSE}
n_sim = 500 # 500 simulations (possibly not enough simulations, but for time...)

LikRatParBoot <- replicate(n = n_sim, LikRatioSim())
```

Ignore the `boundary (singular) fit: see help('isSingular')` for the moment.

Our distribution of LR values under the parametric bootstrap (simulations of data under the reduced model)
```{r}
hist(LikRatParBoot, 
     breaks = 30, main = NULL,
     xlab = "Likelihood ratio from parametric bootstrap")
```

Let's do this again, but adding our likelihood ratio of the two models from the observed data
```{r}
hist(LikRatParBoot, 
     breaks = 30, xlim = c(0, 135), main = NULL,
     xlab = "Likelihood ratio from parametric bootstrap")

abline(v = LR_model, col = "red", lty = 2)
```

And we can calculate the p-value (up to the limits of the number, n_sim we performed) as follows
```{r}
(length(LikRatParBoot[LikRatParBoot >= LR_model]) + 1)/n_sim
```

The parametric bootstrap makes very strong assumptions, so do evaluate whether this assumptions will be problematic with your data and model.


## Note on using REML vs ML (Resticted Maximum Likelihood vs Maximum Likelihood)

- If you are comparing models that only differ in random effects, use `REML = TRUE`

- If you are comparing models that only differ in fixed effects, use `REML = FALSE` (i.e you are using regular maximum likelihood)

- If you are comparing models that differ in both, use `REML = FALSE`. The variance components for the random effects will be biased, but at least fixed effects will be be interpretable!


## pbkrtest library can automate the tasks

- clean syntax, and pretty fast!

```{r}
PB_bootie <- PBmodcomp(largeModel = model_1_REML_us, 
                       smallModel = model_reduced_REML, 
                       nsim = 999, cl = 4) # set cl for number of clusters/cores

PB_bootie$test["PBtest",] # extract the parametric bootstrap 
```


## Using the parametric bootstrap instead of profiling to get confidence intervals on your estimates

For some complex models you will find that profiling of the likelihood surface can run into roadblocks and fail.

While the use of the parametric bootstrap makes strong assumptions regarding distributions the data is coming from, if all else fails, it is a reasonable alternative. 
(Ian personal note, I would use a fully Bayesian approach, with somewhat regularizing priors, or a hierarchical, non-parametric bootstrap (with strata) preferentially, but the PB is an alternative).



**NOTE: WE ARE NOW ONLY SIMULATING UNDER THE FULL MODEL, BASED ON THE FULL MODEL**

The easy way (thanks Ben!). In lme4 there is a `confint.merMod`. We used this above as the default is to use ML profiles, but it can use a NP-bootstrap as well.

```{r}
PB_CIs <- confint(model_1_REML_us, level = 0.95, 
                   method = "boot", boot.type = "perc", nsim = 1000)

PB_CIs
```


I assume under the hood it is doing something like (using `bootMer` via `simulate.merMod` perhaps?)

```{r}
pull_out_estimates <- function(.) {
                         c(beta = fixef(.), 
                           sigma = sigma(.), 
                           sig01 = (unlist(VarCorr(.))))} 
              # as variances and correlations I think. Double check with BB.


PB_boot_full_model <- bootMer(model_1_REML_us,
                              FUN = pull_out_estimates,
                              nsim = 1000,
                              use.u = FALSE,
                              type = "parametric")


PB_boot_full_model

confint(PB_boot_full_model, type = "perc")
```


For more information take a look at `?confint.bootMer` and `?bootMer`.



We could also re-write the function to do so ourselves (but instead of fitting both reduced and full model, we just do the full model). However, these helper functions, are well... helpful!!!!



### Other stuff

For PB based LRT and other inferential approaches see also RLRsim, pbkrtest, lmerTest, arm::se.ranef()
Other useful libraries, r2glmm, r2beta.
robust lmm is also worth looking into.


```{r}
sessionInfo()
```

